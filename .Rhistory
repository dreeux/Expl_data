library(data.table)
library(zoo)
library(forecast)
library(ggplot2)
library(dplyr)

test <- fread("C:/Users/amulya/Documents/Kaggle/forecast/test.csv")

train <- fread("C:/Users/amulya/Documents/Kaggle/forecast/train.csv")

store <- fread("C:/Users/amulya/Documents/Kaggle/forecast/store.csv")

str(train)

str(test)

str(store)

train[, Date := as.Date(Date)]

test[, Date := as.Date(Date)]

train <- train[order(Date)]

test <- test[order(Date)]

#combine data

train <- data.frame(train) ; test <- data.frame(test)

train <- left_join(train, store, by = "Store")

train <- filter(train, Open == 1) 

ggplot(train, aes( x = Customers, y =Sales)) + geom_point() + labs(title = "Customers and Sales")

names(train)

per_customer <- train[ Sales != 0, mean(Sales / Customers), by = "Store"]

quantile(per_customer$V1)

ggplot(per_customer, aes( y = V1 , x = Store )) + geom_line()

ggplot(per_customer, aes( y = (V1) , x = Store )) + geom_jitter(alpha = 0.2) + 
geom_boxplot(colour = "Blue", outlier.colour = "black", fill = NA) + labs( y = "Revenue per User")

per_customer[V1 < 4 | V1 > 15]

Outliers <- train[Store %in% c(158, 353, 455, 612, 769, 842)]

ggplot(train[Store %in% c(158, 353, 455, 612, 769, 842)], aes( x = Customers, y = Sales)) + geom_point()


plot(Outliers$Date, type = "l")

ggplot(Outliers[Sales != 0], aes(x = factor(SchoolHoliday), y = Sales)) +
  geom_jitter(alpha = 0.4) +
  geom_boxplot(color = "yellow", outlier.colour = NA, fill = NA)

ggplot(train[train$Sales != 0 & train$Customers != 0],
       aes(x = factor(Promo), y = Sales)) + 
  geom_jitter(alpha = 0.1) +
  geom_boxplot(color = "yellow", outlier.colour = NA, fill = NA)


ggplot(train[train$Sales != 0 & train$Customers != 0],
       aes(x = factor(Promo), y = Customers)) + 
  geom_jitter(alpha = 0.1) +
  geom_boxplot(color = "yellow", outlier.colour = NA, fill = NA)


plot(train[Store == 972, Sales], ylab = "Sales", xlab = "Days", main = "Store 972")


ggplot(train[Store == 85], 
       aes(x = Date, y = Sales, 
           color = factor(DayOfWeek == 7), shape = factor(DayOfWeek == 7))) + 
  geom_point(size = 3) + ggtitle("Sales of store 85 (True if sunday)")


ggplot(train[Sales != 0],
       aes(x = factor(DayOfWeek), y = Sales)) + 
  geom_jitter(alpha = 0.1) + 
  geom_boxplot(color = "yellow", outlier.colour = NA, fill = NA)


ggplot(train_store[Sales != 0], 
       aes(x = as.Date(Date), y = Sales, color = factor(StoreType))) + 
  geom_smooth(size = 2)


ggplot(train_store[Sales != 0],
       aes(x = factor(!is.na(CompetitionOpenSinceYear)), y = Sales)) +
  geom_jitter(alpha = 0.1) +
  geom_boxplot(color = "yellow", outlier.colour = NA, fill = NA) +
  ggtitle("Any competition?")


ggplot(train[train$Sales != 0,], aes(x = factor(DayOfWeek), y = Sales)) + 
  geom_jitter(alpha = 0.1) + 
  geom_boxplot(colour = "orange", outlier.colour = NA, fill = NA) + 
  labs(title = "Sales by Day and Year") +
  facet_wrap(~ Year)


ggplot(train, aes(x = Customers, y = Sales)) + 
  geom_point(aes(colour = StoreType)) + 
  labs(title = "Customers and Sales")


ggplot(train, aes(x = Customers, y = Sales)) +
  geom_point(aes(colour = Promo)) + 
  labs(title = "Promo on Sales")

#show pay day effect 
#extend by describing sales between days

###########################################################################################################################

This was a pretty simple strategy that scored in the top 25 all by itself, giving a LB score of 0.40327.

XGB Boost as main workhorse.
Created 10-fold training self-predictions
Added in different combinations of features when building models.
Used genetic algorithm ensemble (with replacement).
For features, I used obvious ones (sum of the row, number of non-zero, max of the row, number of 1s, 2s, 3s, and position of max value) as well as different manifold algorithms (e.g., t-SNE).

Some distance measures worked better than others, e.g., Correlation distance. Here's a gif I made showing how it separates. (I'll be providing a Kaggle script for this soon.)

http://i.imgur.com/T78VdKw.gifv

The gif is 45 features cross plotted against each other. For each row, I calculated the distance between that row and all the rows in Class 1, Class 2, etc. That gave me a distribution of distances for that row. Then I calculated the 10, 25, 50, 75, 90th percentile of those distributions for each class. In other words, for a random row, I'm calculating the distribution of how near it is to all the points in each class. (9 classes x 5 percentiles)

I also used previous best model predictions as features.

For the Genetic Algorithm, I used DEAP where the gene was a vector length 20 composed of different model numbers.

A trick I found useful:

I always used my best LB submission to date act as a pseudo ground truth, and compared new submissions against that (loss typically being around 0.20). If I ran a model that had a good training loss, but the "pseudo score" got significantly worse, that meant I was over fitting.

In other words, if my local loss improved to 0.390 but my pseudo-loss jumped to 0.25, I knew I overfit.

EDIT: I uploaded a youtube video of the feature cross plots that is better than the gif.
