rm(list = ls())

require("h2o") ;require("h2oEnsemble"); require("SuperLearner"); require("cvAUC")

library(readr); library(xgboost); library(doParallel); library(caret); library(Metrics)

set.seed(8675309)

train <- read_csv("D:/kaggle/Springleaf/DATA/CSV/train.csv")

# save ID and response

train_target <- train$target

train$target <- NULL

train_ID <- train$ID

train$ID <- NULL

train$VAR_0241 <- as.numeric(as.character(train$VAR_0241))

test <- read_csv("D:/kaggle/Springleaf/DATA/CSV/test.csv")

# save ID and response

test_ID <- test$ID

test$ID <- NULL

test$VAR_0241 <- as.numeric(as.character(test$VAR_0241))

print(dim(train)); print(dim(test))

datecolumns = c("VAR_0073", "VAR_0075", "VAR_0156", "VAR_0157", "VAR_0158",

"VAR_0159", "VAR_0166", "VAR_0167", "VAR_0168", "VAR_0176",

"VAR_0177", "VAR_0178", "VAR_0179", "VAR_0204", "VAR_0217")

train_cropped <- train[datecolumns]

train_cc <- data.frame(apply(train_cropped, 2, function(x) as.double(strptime(x,

format='%d%b%y:%H:%M:%S', tz="UTC"))))

for (dc in datecolumns){

train[dc] <- NULL

train[dc] <- train_cc[dc]

}

train_cc <- NULL

train_cropped <- NULL

gc()

test_cropped <- test[datecolumns]

test_cc <- data.frame(apply(test_cropped, 2, function(x) as.double(strptime(x,

format='%d%b%y:%H:%M:%S', tz="UTC"))))

for (dc in datecolumns){

test[dc] <- NULL

test[dc] <- test_cc[dc]

}

test_cc <- NULL

test_cropped <- NULL

gc()

feature.names <- names(train)[1:ncol(train)]

for (f in feature.names) {

if (class(train[[f]])=="character") {

levels <- unique(c(train[[f]], test[[f]]))

train[[f]] <- as.integer(factor(train[[f]], levels=levels))

test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))

}

}

print(dim(train)); print(dim(test))

train_pre <-  preProcess(train[datecolumns], method = ("BoxCox"))

train_pre_pred <- predict(train_pre, train[datecolumns])

train[datecolumns] <- train_pre_pred

test_pre <-  preProcess(test[datecolumns], method = ("BoxCox"))

test_pre_pred <- predict(test_pre, test[datecolumns])

test[datecolumns] <- test_pre_pred

print(dim(train)); print(dim(test))

nzv <- nearZeroVar(train)

nzv_test <- nearZeroVar(test)

train <- train[, -nzv]

test <- test[, -nzv_test]

print(dim(train)); print(dim(test))

train[is.na(train)] <- 0

test[is.na(test)]   <- 0

gc()

localH2O <- h2o.init(max_mem_size = "10g")

train_target <- as.factor(train_target)

train$ID <- train_ID; train$target <- train_target

test$ID <- test_ID

feature.names <- names(train[1:(ncol(train) -2) ])

train.hex <- as.h2o(localH2O, object = train)

test.hex <- as.h2o(localH2O, object = test)

split <- h2o.runif(train.hex, seed = 1234)

training_frame <- h2o.assign(train.hex[split<0.8,], "training_frame")

validation_frame <- h2o.assign(train.hex[split>=0.8,], "validation_frame")

learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper", 
   "h2o.gbm.wrapper", "h2o.deeplearning.wrapper")

metalearner <- "h2o.deeplearning.wrapper"

family <- "binomial"

fit <- h2o.ensemble(x = feature.names , y = "target", 
                                   
		training_frame = training_frame, 
                    
		family = family, 
                    	
		learner = learner, 
                    	
		metalearner = metalearner,
                    	
		cvControl = list(V = 5, shuffle = 		
		TRUE))

pred <- predict.h2o.ensemble(fit, validation_frame)

predictions <- as.data.frame(pred$pred)[,3]  

labels <- as.data.frame(validation_frame[,c(y)])[,1]

cvAUC::AUC(predictions = predictions , labels = labels)

L <- length(learner)

auc <- sapply(seq(L), function(l) cvAUC::AUC(predictions = 

as.data.frame(pred$basepred)[,l], labels = labels)) 

data.frame(learner, auc)



##############################################################################################################################


require("h2o") ;require("h2oEnsemble"); require("SuperLearner"); require("cvAUC")

library(readr); library(xgboost); library(doParallel); library(caret); library(Metrics)

rm(list = ls())

train <- read_csv("D:/kaggle/Springleaf/DATA/modify/training.csv")

test <- read_csv("D:/kaggle/Springleaf/DATA/modify/testing.csv")

set.seed(8675309)

train_raw <- read_csv("D:/kaggle/Springleaf/DATA/CSV/train.csv")

# save ID and response

train_target <- train_raw$target

train_ID <- train_raw$ID

train$VAR_0241 <- as.numeric(as.character(train$VAR_0241))

test_raw <- read_csv("D:/kaggle/Springleaf/DATA/CSV/test.csv")

# save ID and response

test_ID <- test_raw$ID

test$VAR_0241 <- as.numeric(as.character(test$VAR_0241))

print(dim(train)); print(dim(test))

rm(train_raw); rm(test_raw)

gc()

localH2O <- h2o.init(max_mem_size = "10g")

train_target <- as.factor(train_target)

train$ID <- train_ID; train$target <- train_target

test$ID <- test_ID

feature.names <- names(train[1:(ncol(train) -2) ])

train.hex <- as.h2o(localH2O, object = train)

test.hex <- as.h2o(localH2O, object = test)

split <- h2o.runif(train.hex, seed = 1234)

training_frame <- h2o.assign(train.hex[split<0.9,], "training_frame")

validation_frame <- h2o.assign(train.hex[split>=0.9,], "validation_frame")

###############################################################################


#rf

start <- Sys.time()

test_rf <- h2o.randomForest(x = feature.names,
                            
                            y = "target",
                            
                            training_frame = training_frame, 
                            
                            validation_frame = validation_frame,
                            
                            model_id = "rf_09292015", 
                            
                            ntrees = 2000, 
                            
                            max_depth = 10, 
                            
                            binomial_double_trees = T, 
                            
                            balance_classes = T, 
                            
                            seed = 8675309 
                            
)

rf_time <- Sys.time() - start #17.68586 mins #Total - 165.xx mins

######################################################################################################

test_rf@model$training_metrics@metrics$AUC

test_rf@model$validation_metrics@metrics$AUC

h2o.performance(model = test_rf, validation_frame)

pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)

pred_rf <- as.data.frame(pred_rf)

submission <- data.frame(ID = test_ID)

submission$target <- pred_rf$p1

write_csv(submission, "D:/kaggle/Springleaf/SUBMISSION/rf_09292015.csv")


h2o.saveModel(object = test_rf, path = "D:/kaggle/Springleaf/rf_09272015")


##PLOT METHOD TO FIND SEE HOW AUC VARIES OVER DIFFERENT TUNING PARAMETERS

test_rf@model$scoring_history$number_of_trees

test_rf@model$scoring_history$training_AUC

test_rf@model$scoring_history$validation_AUC

#Didn`t the graph much

plot(test_rf@model$training_metrics, type = "roc", 
     
     col = "black", typ = "b")

par(new = T)

plot(test_rf@model$validation_metrics, type = "roc", 
     
     col = "red", typ = "b", axes = FALSE)

#######################################################################

#Might use a interactive graph to make it more easier to interpret


plot(x = test_rf@model$scoring_history$number_of_trees,
     
     y = test_rf@model$scoring_history$training_AUC, col = "red",
     
     typ = "b", xlab = "Number of Trees", ylab = "AUC"
)

par(new = T)


plot(x = test_rf@model$scoring_history$number_of_trees,
     
     y = test_rf@model$scoring_history$validation_AUC, col = "black",
     
     typ = "b", axes = F, xlab = "Number of Trees", ylab = "AUC"
)



##############################################################################

#GLM

start <- Sys.time()

test_glm <- h2o.glm( x = feature.names,
                     
                     y = "target",
                     
                     training_frame = training_frame,
                     
                     validation_frame = validation_frame,
                     
                     family = "binomial",
                     
                     lambda_search = TRUE,
                     
                     nlambdas = 10, 
                     
                     model_id = "glm_test", 
                     
                     solver = "L_BFGS",
                     
                     keep_cross_validation_predictions = T,
                     
                     alpha = c(0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1), 
                     
                     link = "logit", 
                     
                     standardize = T 
)

glm_time <- Sys.time() - start #5.086208 mins


h2o.saveModel(object = test_glm, path = "D:\\kaggle\\Springleaf\\glm_09272015")

test_glm@model$training_metrics@metrics$AUC

test_glm@model$validation_metrics@metrics$AUC

#code working

#add validation dataset to check during training 

#PREDICTIONS ON TEST DATASET

#SAVE THE PREDICTION VECTOR IN A CSV FILE(09272015_glm)

##############################################################################


########################################################################################################

#GBM

start <- Sys.time()

test_gbm <- h2o.gbm(x = feature.names,
                    
                    y = "target",
                    
                    training_frame = training_frame,
                    
                    validation_frame = validation_frame,
                    
                    model_id = "gbm_09272015", 
                    
                    ntrees =  100, 
                    
                    max_depth = 20, 
                    
                    learn_rate = 0.014, 
                    
                    seed = 8675309, 
                    
                    balance_classes = T, 
                    
                    min_rows = 9 
)


gbm_time <- Sys.time() - start #1.496173 hours

h2o.saveModel(object = test_gbm, path = "D://kaggle//Springleaf//gbm_09282015")

#code working


#add validation dataset to check during training 

test_gbm@model$training_metrics@metrics$AUC #Now showing AUC on train

test_gbm@model$scoring_history$number_of_trees

test_gbm@model$scoring_history$training_AUC

test_gbm@model$scoring_history$validation_AUC



########################################################################################################


#####################################################################################################

test_gbm@model$training_metrics@metrics$AUC

h2o.performance(model = test_gbm, validation_frame)

pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)

submission <- data.frame(ID = test_ID)

submission$target <- pred_gbm

write_csv(submission, "D:/kaggle/Springleaf/SUBMISSION/gbm_09272015.csv")

#####################################################################################################

test_glm@model$training_metrics@metrics$AUC

h2o.performance(model = test_glm, validation_frame)

pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)

pred_glm <- as.data.frame(pred_glm)

submission <- data.frame(ID = test_ID)

submission$target <- pred_glm$p1

write_csv(submission, "D:/kaggle/Springleaf/SUBMISSION/glm_09272015.csv")


###################################################################################################





### read_data


set.seed(02*07*2016)

# read in the data file--------------------------------------------------------------------------------

require(data.table); require(xgboost); require(h2o); require(caret)


train <- fread("C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\Data\\train.csv", data.table = F)

test  <- fread("C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\Data\\test.csv", data.table = F)

train_id <- train$Id

train$Id <- NULL;

id <- test$Id; test$Id <- NULL

response <- train$Response; train$Response <- NULL


tmp <- rbind(train, test)


row_NA <- apply(tmp, 1, function(x) sum(is.na(x)))

tmp$row_NA <- row_NA


# new feature engg found in high performing script----------------------------------------------------------

first <- c()

char <- as.character(tmp$Product_Info_2)

for(i in 1:length(tmp$Product_Info_2))
  
{

first[i] <- substr(char[i], 1, 1) 

}


tmp$first <- first


second <- c()

for(i in 1:length(tmp$Product_Info_2))
  
{
  
  second[i] <- substr(char[i], 2, 2) 
  
}


tmp$second <- second


# dummify varible------------------------------------------------------------------------------

dummy <- c("Product_Info_2", "first", "second")

tmp_dummy <- data.frame(tmp[, dummy])


for(i in 1:ncol(tmp_dummy))
  
  {


  tmp_dummy[ , i] <- as.factor(tmp_dummy[ , i])

}

dummies <- dummyVars( ~ ., data = tmp_dummy)

gc()

tmp_dummy <- predict(dummies, newdata = tmp_dummy)

tmp_dummy <- data.frame(tmp_dummy)

dim(tmp_dummy)


# count the number of keywords row wise--------------------------------------------------------

keywords <- paste("Medical_Keyword_", 1:48, sep="")

tmp_count <- tmp[, keywords]


count <- apply(tmp_count, 1, function(x) sum(x))

tmp$count <- count


##############################################################################################

tmp[is.na(tmp)] <- -1


# interaction features-------------------------------------------------------------------------


tmp_int <- tmp[ , c("Ins_Age", "BMI")]

for (i in 1:ncol(tmp_int)) {
  
  for (j in (i + 1) : (ncol(tmp_int) + 1)) {
    
    #    a = i; b= j
    
    var.x <- colnames(tmp_int)[i]
    
    var.y <- colnames(tmp_int)[j]
    
    var.new <- paste0(var.x, '_plus_', var.y)
    
    tmp_int[ , paste0(var.new)] <- tmp_int[, i] + tmp_int[, j]
    
  }
}


gc()


tmp$Medical_History_10 <- NULL

tmp$Medical_History_24 <- NULL


############################################################################################################

feature.names <- names(tmp)

for (f in feature.names) {
  
  if (class(tmp[[f]])=="character") {
    
    levels <- unique(c(tmp[[f]]))
    
    tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))
    
  }
}


tmp_new <- cbind(tmp, tmp_dummy, tmp_int)

tmp_new <- tmp_new[ , !(names(tmp_new) %in% c("Product_Info_2", "first", "second"))]


train <- tmp_new[c(1:59381),]

test <- tmp_new[c(59382:79146),]

require(readr)

write_csv(train, "C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\Data\\train_02072016.csv")

write_csv(test, "C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\Data\\test_02072016.csv")

#####

xgb


# create folds------------------------------------------------------------------------------------------

dataset_blend_train = matrix(0, nrow(train), 1)

dataset_blend_test_j = matrix(0, nrow(test), 3)

dataset_blend_test = matrix(0, nrow(test), 1)

# start iteration loop---------------------------------------------------------------------------------
# 
# for(j in 1:5)
#   
# {
  
  j= 1
  
  print(paste("starting xgboost iteration ; number :", j))
  
  set.seed(2*07*2016*j)
  
  require(caret)
  
  skf = createFolds(response, k = 3)
  
  print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
  
  # start fold loop------------------------------------------------------------------------------------
  
  ### Loop over the folds
  
  i <- 0
  
  for (sk in skf) {
    
    i <- i + 1
    
    print(paste("Fold", i))
    
    ### Extract and fit the train/test section for each fold
    
    tmp_train <- unlist(skf[i])
    
    x_train = train[-tmp_train,]
    
    y_train = response[-tmp_train]
    
    x_test  = train[tmp_train,]
    
    y_test  = response[tmp_train]
    
    
    feature.names <- names(train)
    
    dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)
    
#     param <- list( objective           = "count:poisson", # objective           = "reg:linear"
#                    
#                    depth = 21,
#                    
#                    min_child_weight = 3,
#                    
#                    subsample = 0.71,
#                    
#                    eta = 0.01,
#                    
#                    silent = 0
#     )
# 
#     
    
    param <- list( objective           = "reg:linear", 
                   
                   depth = 21,
                   
                   min_child_weight = 3,
                   
                   subsample = 0.71,
                   
                   eta = 0.01,
                   
                   silent = 0
    )    
    
    
    
    
    # start training------------------------------------------------------------------------------
    
    print(paste("training xgboost for iteration :", j, "Fold ; number :", i))
    
    mod <- xgb.train(   params              = param,
                        
                        booster = "gbtree",
                        
                        data                = dtrain,
                        
                        nrounds             = 3000,
                        
                        verbose             = 1,
                        
                        maximize            = F
                        
    )
    
    dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)
    
    
    
    print(paste("predicting xgboost for test set iteration:", j, " ; Fold :", i))
    
    dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)
    
  }
  
  dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
  
#}

require(readr)

write_csv(data.frame(dataset_blend_train), "C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\blend\\bag\\xgb\\blend_train_xgb_02072016.csv")

write_csv(data.frame(dataset_blend_test), "C:\\Users\\amulya\\Documents\\Kaggle\\PRUDENTIAL\\blend\\bag\\xgb\\blend_test_xgb_02072016.csv")
